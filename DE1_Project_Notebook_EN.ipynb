{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ad21fb",
   "metadata": {},
   "source": [
    "# DE1 — Final Project Notebook\n",
    "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
    "---\n",
    "\n",
    "This is the primary executable artifact. Fill config, run baseline, then optimized pipeline, and record evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5be1e5",
   "metadata": {},
   "source": [
    "## 0. Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, pathlib, datetime\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "with open(\"project/de1_project_config.yml\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"de1-project\").getOrCreate()\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ccb2c",
   "metadata": {},
   "source": [
    "## 1. Bronze — landing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9727ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_glob = CFG[\"paths\"][\"raw_csv_glob\"]\n",
    "bronze = CFG[\"paths\"][\"bronze\"]\n",
    "proof = CFG[\"paths\"][\"proof\"]\n",
    "\n",
    "df_raw = (spark.read.option(\"header\",\"true\").csv(raw_glob))\n",
    "df_raw.write.mode(\"overwrite\").csv(bronze)  # keep raw as CSV copy\n",
    "print(\"Bronze written:\", bronze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bcbf7",
   "metadata": {},
   "source": [
    "## 2. Silver — cleaning and typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6457790",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver = CFG[\"paths\"][\"silver\"]\n",
    "\n",
    "# Example typing; adapt to dataset\n",
    "from pyspark.sql import functions as F, types as T\n",
    "df_silver = (df_raw\n",
    "    .withColumn(\"metric\", F.col(\"metric\").cast(\"double\"))\n",
    "    .withColumn(\"date\", F.to_date(\"date\"))\n",
    "    .dropna(subset=[\"metric\",\"date\"]))\n",
    "\n",
    "df_silver.write.mode(\"overwrite\").parquet(silver)\n",
    "print(\"Silver written:\", silver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee46bcc",
   "metadata": {},
   "source": [
    "## 3. Gold — analytics tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = CFG[\"paths\"][\"gold\"]\n",
    "partition_by = CFG[\"layout\"][\"partition_by\"]\n",
    "\n",
    "# Example gold Q1\n",
    "gold_q1 = (df_silver.groupBy(\"date\").agg(F.sum(\"metric\").alias(\"sum_metric\")))\n",
    "(gold_q1.write.mode(\"overwrite\").partitionBy(*partition_by).parquet(f\"{gold}/q1_daily\"))\n",
    "\n",
    "print(\"Gold written:\", gold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235c14d",
   "metadata": {},
   "source": [
    "## 4. Baseline plans and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3396cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime as _dt, pathlib\n",
    "pathlib.Path(proof).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Example baseline plan\n",
    "plan = gold_q1._jdf.queryExecution().executedPlan().toString()\n",
    "with open(f\"{proof}/baseline_q1_plan.txt\",\"w\") as f:\n",
    "    f.write(str(_dt.datetime.now())+\"\\n\")\n",
    "    f.write(plan)\n",
    "print(\"Saved baseline plan. Record Spark UI metrics now.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65c5fb",
   "metadata": {},
   "source": [
    "## 5. Optimization — layout and joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: narrow projection and pre‑aggregation before write\n",
    "df_silver_min = df_silver.select(\"date\",\"metric\")\n",
    "gold_q1_opt = (df_silver_min.groupBy(\"date\").agg(F.sum(\"metric\").alias(\"sum_metric\")))\n",
    "gold_q1_opt.write.mode(\"overwrite\").partitionBy(*partition_by).parquet(f\"{gold}/q1_daily_opt\")\n",
    "\n",
    "plan_opt = gold_q1_opt._jdf.queryExecution().executedPlan().toString()\n",
    "with open(f\"{proof}/optimized_q1_plan.txt\",\"w\") as f:\n",
    "    f.write(str(_dt.datetime.now())+\"\\n\")\n",
    "    f.write(plan_opt)\n",
    "print(\"Saved optimized plan. Record Spark UI metrics now.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19863c22",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
