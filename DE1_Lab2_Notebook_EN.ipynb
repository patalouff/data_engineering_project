{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108c50e7",
   "metadata": {},
   "source": [
    "# DE1 — Lab 2: PostgreSQL → Star Schema ETL\n",
    "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
    "---\n",
    "\n",
    "\n",
    "Execute all cells. Attach evidence and fill metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598a373",
   "metadata": {},
   "source": [
    "## 0. Setup and schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "spark = SparkSession.builder.appName(\"de1-lab2\").getOrCreate()\n",
    "base = \"data/\"\n",
    "# Explicit schemas\n",
    "customers_schema = T.StructType([\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), True),\n",
    "    T.StructField(\"email\", T.StringType(), True),\n",
    "    T.StructField(\"created_at\", T.TimestampType(), True),\n",
    "])\n",
    "brands_schema = T.StructType([\n",
    "    T.StructField(\"brand_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"brand_name\", T.StringType(), True),\n",
    "])\n",
    "categories_schema = T.StructType([\n",
    "    T.StructField(\"category_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"category_name\", T.StringType(), True),\n",
    "])\n",
    "products_schema = T.StructType([\n",
    "    T.StructField(\"product_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"product_name\", T.StringType(), True),\n",
    "    T.StructField(\"brand_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"category_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"price\", T.DoubleType(), True),\n",
    "])\n",
    "orders_schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"order_date\", T.TimestampType(), True),\n",
    "])\n",
    "order_items_schema = T.StructType([\n",
    "    T.StructField(\"order_item_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"order_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"product_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"quantity\", T.IntegerType(), True),\n",
    "    T.StructField(\"unit_price\", T.DoubleType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb061866",
   "metadata": {},
   "source": [
    "## 1. Ingest operational tables (from CSV exports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b011c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = spark.read.schema(customers_schema).option(\"header\",\"true\").csv(base+\"lab2_customers.csv\")\n",
    "brands = spark.read.schema(brands_schema).option(\"header\",\"true\").csv(base+\"lab2_brands.csv\")\n",
    "categories = spark.read.schema(categories_schema).option(\"header\",\"true\").csv(base+\"lab2_categories.csv\")\n",
    "products = spark.read.schema(products_schema).option(\"header\",\"true\").csv(base+\"lab2_products.csv\")\n",
    "orders = spark.read.schema(orders_schema).option(\"header\",\"true\").csv(base+\"lab2_orders.csv\")\n",
    "order_items = spark.read.schema(order_items_schema).option(\"header\",\"true\").csv(base+\"lab2_order_items.csv\")\n",
    "\n",
    "for name, df in [(\"customers\",customers),(\"brands\",brands),(\"categories\",categories),(\"products\",products),(\"orders\",orders),(\"order_items\",order_items)]:\n",
    "    print(name, df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56647b6",
   "metadata": {},
   "source": [
    "### Evidence: ingestion plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest = orders.join(order_items, \"order_id\").select(\"order_id\").distinct()\n",
    "ingest.explain(\"formatted\")\n",
    "from datetime import datetime as _dt\n",
    "import pathlib\n",
    "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
    "with open(\"proof/plan_ingest.txt\",\"w\") as f:\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(ingest._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_ingest.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2cabf1",
   "metadata": {},
   "source": [
    "## 2. Surrogate key function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk(cols):\n",
    "    # stable 64-bit positive surrogate key from natural keys\n",
    "    return F.abs(F.xxhash64(*[F.col(c) for c in cols]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121fcea1",
   "metadata": {},
   "source": [
    "## 3. Build dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11477c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_customer = customers.select(\n",
    "    sk([\"customer_id\"]).alias(\"customer_sk\"),\n",
    "    \"customer_id\",\"name\",\"email\",\"created_at\"\n",
    ")\n",
    "\n",
    "dim_brand = brands.select(\n",
    "    sk([\"brand_id\"]).alias(\"brand_sk\"),\n",
    "    \"brand_id\",\"brand_name\"\n",
    ")\n",
    "\n",
    "dim_category = categories.select(\n",
    "    sk([\"category_id\"]).alias(\"category_sk\"),\n",
    "    \"category_id\",\"category_name\"\n",
    ")\n",
    "\n",
    "dim_product = products.select(\n",
    "    sk([\"product_id\"]).alias(\"product_sk\"),\n",
    "    \"product_id\",\"product_name\",\n",
    "    sk([\"brand_id\"]).alias(\"brand_sk\"),\n",
    "    sk([\"category_id\"]).alias(\"category_sk\"),\n",
    "    \"price\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725023cc",
   "metadata": {},
   "source": [
    "## 4. Build date dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72028d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window as W\n",
    "dates = orders.select(F.to_date(\"order_date\").alias(\"date\")).distinct()\n",
    "dim_date = dates.select(\n",
    "    sk([\"date\"]).alias(\"date_sk\"),\n",
    "    F.col(\"date\"),\n",
    "    F.year(\"date\").alias(\"year\"),\n",
    "    F.month(\"date\").alias(\"month\"),\n",
    "    F.dayofmonth(\"date\").alias(\"day\"),\n",
    "    F.date_format(\"date\",\"E\").alias(\"dow\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066f8a8",
   "metadata": {},
   "source": [
    "## 5. Build fact_sales with broadcast joins where appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oi = order_items.alias(\"oi\")\n",
    "p = products.alias(\"p\")\n",
    "o = orders.alias(\"o\")\n",
    "c = customers.alias(\"c\")\n",
    "\n",
    "# Join with small dimensions using DF copies to compute SKs, then broadcast dims by size heuristic\n",
    "df_fact = (oi\n",
    "    .join(p, F.col(\"oi.product_id\")==F.col(\"p.product_id\"))\n",
    "    .join(o, \"order_id\")\n",
    "    .join(c, \"customer_id\")\n",
    "    .withColumn(\"date\", F.to_date(\"order_date\"))\n",
    ")\n",
    "\n",
    "# Attach surrogate keys\n",
    "df_fact = (df_fact\n",
    "    .withColumn(\"date_sk\", sk([\"date\"]))\n",
    "    .withColumn(\"customer_sk\", sk([\"customer_id\"]))\n",
    "    .withColumn(\"product_sk\", sk([\"product_id\"]))\n",
    "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
    "    .withColumn(\"subtotal\", F.col(\"quantity\")*F.col(\"unit_price\"))\n",
    "    .withColumn(\"year\", F.year(\"date\"))\n",
    "    .withColumn(\"month\", F.month(\"date\"))\n",
    "    .select(\"order_id\",\"date_sk\",\"customer_sk\",\"product_sk\",\"quantity\",\"unit_price\",\"subtotal\",\"year\",\"month\")\n",
    ")\n",
    "\n",
    "df_fact.explain(\"formatted\")\n",
    "with open(\"proof/plan_fact_join.txt\",\"w\") as f:\n",
    "    from datetime import datetime as _dt\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(df_fact._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_fact_join.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61d5ad",
   "metadata": {},
   "source": [
    "## 6. Write Parquet outputs (partitioned by year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094896ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_out = \"outputs/lab2\"\n",
    "(dim_customer.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_customer\"))\n",
    "(dim_brand.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_brand\"))\n",
    "(dim_category.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_category\"))\n",
    "(dim_product.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_product\"))\n",
    "(dim_date.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_date\"))\n",
    "(df_fact.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(f\"{base_out}/fact_sales\"))\n",
    "print(\"Parquet written under outputs/lab2/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50317f3",
   "metadata": {},
   "source": [
    "## 7. Plan comparison: projection and layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case A: join and then project\n",
    "a = (orders.join(order_items, \"order_id\")\n",
    "            .join(products, \"product_id\")\n",
    "            .groupBy(F.to_date(\"order_date\").alias(\"d\"))\n",
    "            .agg(F.sum(F.col(\"quantity\")*F.col(\"price\")).alias(\"gmv\")))\n",
    "a.explain(\"formatted\")\n",
    "_ = a.count()\n",
    "\n",
    "# Case B: project early\n",
    "b = (orders.select(\"order_id\",\"order_date\")\n",
    "            .join(order_items.select(\"order_id\",\"product_id\",\"quantity\"), \"order_id\")\n",
    "            .join(products.select(\"product_id\",\"price\"), \"product_id\")\n",
    "            .groupBy(F.to_date(\"order_date\").alias(\"d\"))\n",
    "            .agg(F.sum(F.col(\"quantity\")*F.col(\"price\")).alias(\"gmv\")))\n",
    "b.explain(\"formatted\")\n",
    "_ = b.count()\n",
    "\n",
    "print(\"Record Spark UI metrics for both runs in lab2_metrics_log.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a4e05",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1191a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
