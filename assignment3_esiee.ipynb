{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ccef370",
   "metadata": {},
   "source": [
    "# ESIEE Paris — Data Engineering I — Assignment 3\n",
    "> Author : Badr TAJINI\n",
    "\n",
    "**Academic year:** 2025–2026  \n",
    "**Program:** Data & Applications - Engineering - (FD)   \n",
    "**Course:** Data Engineering I  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c7a52",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- Analyze with **SQL** and **DataFrames**.\n",
    "- Implement two **RDD means** variants.\n",
    "- Implement **RDD joins** (shuffle and hash).\n",
    "- Record and explain performance observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f7c31",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76cdb5-698f-4369-8c1f-2526d7785b45",
   "metadata": {},
   "source": [
    "Download data files from the following URL:\n",
    "https://www.dropbox.com/scl/fi/7012u693u06dgj95mgq2a/retail_dw_20250826.tar.gz?rlkey=fxyozuoryn951gzwmli5xi2zd&dl=0\n",
    "\n",
    "Unpack somewhere and define the `data_path` accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to path on your local machine.\n",
    "data_path = \"/Users/btajini/de1-work/assignment3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93421e-3ed4-4adc-8dd9-f605b53c8498",
   "metadata": {},
   "source": [
    "The following cell contains setup to measure wall clock time and memory usage. (Don't worry about the details, just run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82905153",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy pandas pyarrow matplotlib scipy\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import psutil  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "print(\"psutil is installed.\")\n",
    "\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import time, os, platform\n",
    "\n",
    "# Try to import optional modules\n",
    "try:\n",
    "    import psutil\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "try:\n",
    "    import resource  # not available on Windows\n",
    "except Exception:\n",
    "    resource = None\n",
    "\n",
    "\n",
    "def _rss_bytes():\n",
    "    \"\"\"Resident Set Size in bytes (cross-platform via psutil if available).\"\"\"\n",
    "    if psutil is not None:\n",
    "        return psutil.Process(os.getpid()).memory_info().rss\n",
    "    # Fallback: unknown RSS → 0 \n",
    "    return 0\n",
    "\n",
    "\n",
    "def _peak_bytes():\n",
    "    \"\"\"\n",
    "    Best-effort peak memory in bytes.\n",
    "    - Windows: psutil peak working set (peak_wset)\n",
    "    - Linux:   resource.ru_maxrss (KB → bytes)\n",
    "    - macOS:   resource.ru_maxrss (bytes)\n",
    "    Fallback to current RSS if unavailable.\n",
    "    \"\"\"\n",
    "    sysname = platform.system()\n",
    "\n",
    "    # Windows path: use psutil peak_wset if present\n",
    "    if sysname == \"Windows\" and psutil is not None:\n",
    "        mi = psutil.Process(os.getpid()).memory_info()\n",
    "        peak = getattr(mi, \"peak_wset\", None)  # should be available on Windows\n",
    "        if peak is not None:\n",
    "            return int(peak)\n",
    "        return int(mi.rss)\n",
    "\n",
    "    # POSIX path: resource may be available\n",
    "    if resource is not None:\n",
    "        try:\n",
    "            ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "            # On Linux ru_maxrss is in kilobytes; on macOS/BSD it is bytes\n",
    "            if sysname == \"Linux\":\n",
    "                return int(ru) * 1024\n",
    "            else:\n",
    "                return int(ru)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Last resort\n",
    "    return _rss_bytes()\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def timemem(line, cell):\n",
    "    \"\"\"\n",
    "    Measure wall time and memory around the execution of this cell.\n",
    "\n",
    "        %%timemem\n",
    "        <your code>\n",
    "\n",
    "    Notes:\n",
    "    - RSS = resident memory after the cell.\n",
    "    - Peak is OS-dependent (see _peak_bytes docstring).\n",
    "    \"\"\"\n",
    "    ip = get_ipython()\n",
    "\n",
    "    rss_before  = _rss_bytes()\n",
    "    peak_before = _peak_bytes()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Execute the cell body\n",
    "    result = ip.run_cell(cell)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    rss_after  = _rss_bytes()\n",
    "    peak_after = _peak_bytes()\n",
    "\n",
    "    wall = t1 - t0\n",
    "    rss_delta_mb  = (rss_after  - rss_before)  / (1024 * 1024)\n",
    "    peak_delta_mb = (peak_after - peak_before) / (1024 * 1024)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Wall time: {wall:.3f} s\")\n",
    "    print(f\"RSS Δ: {rss_delta_mb:+.2f} MB\")\n",
    "    print(f\"Peak memory Δ: {peak_delta_mb:+.2f} MB (OS-dependent)\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55796a4b-ef95-4351-b065-5bb435deda6f",
   "metadata": {},
   "source": [
    "The following code snippet should \"just work\" to initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, os\n",
    "\n",
    "# Change to path on your local machine.\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/jimmylin/Dropbox/workspace/teaching/spark-4.0.0-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "py = sys.executable  # the Python of this notebook (e.g., .../envs/yourenv/bin/python)\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "\n",
    "spark = SparkSession.getActiveSession() or (\n",
    "    SparkSession.builder\n",
    "    .appName(\"A3\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")           \n",
    "    .config(\"spark.sql.shuffle.partitions\",\"400\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.pyspark.driver.python\", py)\n",
    "    .config(\"spark.pyspark.python\", py)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", py)\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7a530",
   "metadata": {},
   "source": [
    "## 2. Loading DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745a229-813a-463b-beee-7b7284b5ed21",
   "metadata": {},
   "source": [
    "Let's load the DataFrames and print out their schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b3d2c-3dfc-4a63-ba32-f3f5dac21e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you should have defined data_path above\n",
    "\n",
    "events_df   = spark.read.parquet(os.path.join(data_path, \"retail_dw_20250826_events\"))\n",
    "products_df = spark.read.parquet(os.path.join(data_path, \"retail_dw_20250826_products\"))\n",
    "brands_df   = spark.read.parquet(os.path.join(data_path, \"retail_dw_20250826_brands\"))\n",
    "\n",
    "events_df.printSchema()\n",
    "products_df.printSchema()\n",
    "brands_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9a33b-fc66-47da-8eb6-18554ff19bdf",
   "metadata": {},
   "source": [
    "How many rows are in each table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70c3a3-4da7-41e9-9e64-ae8f870c29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows in events   table: {events_df.count()}\")\n",
    "print(f\"Number of rows in products table: {products_df.count()}\")\n",
    "print(f\"Number of rows in brands   table: {brands_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b16635-d1c3-4bf9-ba5a-49778d3f1530",
   "metadata": {},
   "source": [
    "We can register the DataFrames as tables and issue SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc244b9-b5d5-42f9-8efe-f7c4ea095499",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.createOrReplaceTempView(\"events\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "brands_df.createOrReplaceTempView(\"brands\")\n",
    "\n",
    "spark.sql('select count(*) from events').show()\n",
    "spark.sql('select count(*) from products').show()\n",
    "spark.sql('select count(*) from brands').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d7aad-277f-495f-965f-831c8d247e4a",
   "metadata": {},
   "source": [
    "As a sanity check, the corresponding values should match: counting the rows in the DataFrame vs. issuing an SQL query to count the number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a37092",
   "metadata": {},
   "source": [
    "## 3. Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93689a-e015-4007-9126-1e10e75d8dfa",
   "metadata": {},
   "source": [
    "Answer Q1 to Q7 below with SQL queries and DataFrame manipulations.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf85258",
   "metadata": {},
   "source": [
    "### 3.1 Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393de9c-1479-480c-9948-881f368a19e3",
   "metadata": {},
   "source": [
    "For session_id `789d3699-028e-4367-b515-b82e2cb5225f`, what was the purchase price?\n",
    "\n",
    "**Hint:** We only care about purchase events.\n",
    "\n",
    "First, do it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fe1d9-5eeb-4cbc-9fb2-5a44b0471da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_31a (keep this id for tracking purposes)\n",
    "\n",
    "# Write your SQL below\n",
    "sql_query = f\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = spark.sql(sql_query)\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b441e-741c-47bd-afea-26b6e259c4ad",
   "metadata": {},
   "source": [
    "Next, do it with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d4457-1f5d-4a90-8974-edc78417c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_31b (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87aa67",
   "metadata": {},
   "source": [
    "### 3.2 Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4f80c-4d3e-4ed5-8a6a-065f7631739f",
   "metadata": {},
   "source": [
    "How many products are sold by the brand \"sokolov\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86f5c8-4dd2-4d9f-9216-ff3a34337fc8",
   "metadata": {},
   "source": [
    "First, do it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b9b4b-81de-4292-bbc8-34f66c3b2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_32a (keep this id for tracking purposes)\n",
    "\n",
    "# Write your SQL below\n",
    "sql_query = f\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = spark.sql(sql_query)\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53249ee1-99aa-41e5-a484-c75342147fd8",
   "metadata": {},
   "source": [
    "Next, do it with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_32b (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4800c",
   "metadata": {},
   "source": [
    "### 3.3 Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e553ce-aa6d-416e-a0f3-6d5b5fe160a0",
   "metadata": {},
   "source": [
    "What is the average purchase price of items purchased from the brand \"febest\"? (Report answer to two digits after the decimal point, i.e., XX.XX.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d59f4-9d27-4b03-9935-aa3aaa797a8d",
   "metadata": {},
   "source": [
    "First, do it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19187c9f-06ce-4364-bf7b-998797ca6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_33a (keep this id for tracking purposes)\n",
    "\n",
    "# Write your SQL below\n",
    "sql_query = f\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = spark.sql(sql_query)\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d1667-6646-4e1b-be7a-c18228861857",
   "metadata": {},
   "source": [
    "Next, do it with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eade2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_33b (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394c731",
   "metadata": {},
   "source": [
    "### 3.4 Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66317c46-94f7-459d-a99b-5e1bba29c6cf",
   "metadata": {},
   "source": [
    "What is the average number of events per user? (Report answer to two digits after the decimal point, i.e., XX.XX.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0888b2-4961-458f-8f58-c9bf29a360dc",
   "metadata": {},
   "source": [
    "First, do it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab447c-b909-4155-a925-d1b01e3420e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_34a (keep this id for tracking purposes)\n",
    "\n",
    "# Write your SQL below\n",
    "sql_query = f\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = spark.sql(sql_query)\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea6bc1-5ee3-4951-8c3c-0c326ebb3b2b",
   "metadata": {},
   "source": [
    "Next, do it with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_34b (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd30e9",
   "metadata": {},
   "source": [
    "### 3.5 Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e011a70-e6e3-46fc-8ff7-041a919dca2b",
   "metadata": {},
   "source": [
    "What are the top 10 (`product_name`, `brand_code`) pairs in terms of revenue? We want the answer rows sorted by revenue in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ca411-eef0-45cf-bd92-99ece1d89510",
   "metadata": {},
   "source": [
    "First, do it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4e788-9576-4122-9d5a-8c1532feef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_35a (keep this id for tracking purposes)\n",
    "\n",
    "# Write your SQL below\n",
    "sql_query = f\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = spark.sql(sql_query)\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcfe9ba-cd8b-4482-9d9d-c5d0501940bf",
   "metadata": {},
   "source": [
    "Next, do it with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_35b (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "\n",
    "results_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab771d9",
   "metadata": {},
   "source": [
    "### 3.6 Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457fcb44-d4a3-4e6e-8dc9-25cc1186b641",
   "metadata": {},
   "source": [
    "Tally up counts of events by hour.\n",
    "More precisely, we want a table with hours 0, 1, ... 23 with the counts of events in that hour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a15e7a-fed4-4229-86a4-5e57d0390581",
   "metadata": {},
   "source": [
    "First, do it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8436405-44fc-4852-9f7a-ad13e26623dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_36a (keep this id for tracking purposes)\n",
    "\n",
    "# Write your SQL below\n",
    "sql_query = f\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = spark.sql(sql_query)\n",
    "\n",
    "results.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5385c-10b5-4768-a88d-d0733ec0d64e",
   "metadata": {},
   "source": [
    "Next, do it with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc5060-a437-48f5-aa50-8602e9f1ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_36b (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "\n",
    "events_by_hour_df.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b294736-e759-4942-939d-5f8545f1ea89",
   "metadata": {},
   "source": [
    "When you run the cell above, `events_by_hour_df` should be something like:\n",
    "\n",
    "```\n",
    "+----+-------+\n",
    "|hour|  count|\n",
    "+----+-------+\n",
    "|   0|    ???|\n",
    "|   1|    ???|\n",
    "  ...\n",
    "|  23|    ???|\n",
    "+----+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42738a71-361d-4508-89ef-a5c255f1d505",
   "metadata": {},
   "source": [
    "Now plot the above DataFrame using `matplotlib`.\n",
    "Here we want a line graph, with hour on the _x_ axis and count on the _y_ axis.\n",
    "\n",
    "**Hint:** use the code below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bcafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_36c (keep this id for tracking purposes)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "events_by_hour_pdf = events_by_hour_df.toPandas()\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10445d63",
   "metadata": {},
   "source": [
    "### 3.7 Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498671fc-16a7-4da2-b9ea-2a18a0564d03",
   "metadata": {},
   "source": [
    "We are going to analyze the \"big\" brands. Find out the average purchase price by brand, and restrict to cases where the average is more than 10K.\n",
    "We want the results sorted by the average purchase price from the largest to smallest value.\n",
    "(Report answers to two digits after the decimal point, i.e., XX.XX, but it's okay if the output only contains one digit after the decimal point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba375938-7e73-4512-8939-b289c78a89d3",
   "metadata": {},
   "source": [
    "First, do it using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec28857-1a72-4541-8260-d28bcfb28dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_37a (keep this id for tracking purposes)\n",
    "\n",
    "# Write your SQL below\n",
    "sql_query = f\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = spark.sql(sql_query)\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9495a-d3f5-4ae8-9698-3e24499d2696",
   "metadata": {},
   "source": [
    "Next, do it with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66223d77-75b5-4168-b656-149c32561e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_37b (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "\n",
    "avg_price_by_brand_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9ef73-ac15-4895-a932-09a48f6b3e2d",
   "metadata": {},
   "source": [
    "When you run the cell above, `avg_price_by_brand_df` should be something like:\n",
    "\n",
    "```\n",
    "+----------+------------------+\n",
    "|brand_code|         avg_price|\n",
    "+----------+------------------+\n",
    "|       ???|               ???|\n",
    "        ...\n",
    "|       ???|               ???|\n",
    "+----------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2546c5d-7c52-4910-8af6-bd6373e6b6c8",
   "metadata": {},
   "source": [
    "Now plot the above DataFrame using `matplotlib`.\n",
    "Here we want a bar chart, with each of the brands as a bar, and the average price on the _y_ axis.\n",
    "\n",
    "**Hint:** use the code below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0887f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_37c (keep this id for tracking purposes)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avg_price_by_brand_pdf = avg_price_by_brand_df.toPandas()\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740f635",
   "metadata": {},
   "source": [
    "## 4. Load RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c86140-8287-4d48-9eff-1b16281174fd",
   "metadata": {},
   "source": [
    "The remaining exercises focus on RDD manipulations.\n",
    "\n",
    "Let's start by loading the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411a973-b7cc-4f31-9e6b-cebaef3196ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RDDs directly from DataFrames (with required repartitions)\n",
    "# type: RDD[Row]\n",
    "events_rdd   = events_df.rdd.repartition(1000)\n",
    "products_rdd = products_df.rdd.repartition(100)\n",
    "brands_rdd   = brands_df.rdd.repartition(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba1e1c0-de17-4e17-985c-3d4329d207b2",
   "metadata": {},
   "source": [
    "You'll need `Row`, so let's make sure we've imported it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291905f-5ec5-442a-b06b-d7fa03e9f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92b878-6f10-4bd6-90c2-624c3bedd41b",
   "metadata": {},
   "source": [
    "## 5. Implementations of Computing Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b6453",
   "metadata": {},
   "source": [
    "In this next exercise, we're going to implement \"computing the mean\" (version 1) and (version 3) in Spark as described in the second lecture **Batch Processing I** (please use ctrl+f to reach the slide with the title : \"Computing the Mean: Version 1\" or \"Computing the Mean: Version 3\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad895c-719c-4c09-b72a-27e890f78f1e",
   "metadata": {},
   "source": [
    "To make the problem more tractable (i.e., to reduce the running times), let's first do a bit of filtering of the `events` table.\n",
    "We'll do this using DataFrames, and then generate an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d9982-816f-49f7-9960-91835e2011a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events_df = (\n",
    "    events_df\n",
    "        .filter((F.col(\"event_type\") == \"purchase\") & F.col(\"price\").isNotNull())\n",
    "        .join(brands_df, on=\"brand_key\")\n",
    ")\n",
    "\n",
    "filtered_events_df.count()\n",
    "\n",
    "print(f\"Number of rows in events          table: {events_df.count()}\")\n",
    "print(f\"Number of rows in filtered events table: {filtered_events_df.count()}\")\n",
    "\n",
    "filtered_events_rdd = filtered_events_df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ffa9e2-e05d-45ab-9885-0a071a6156dd",
   "metadata": {},
   "source": [
    "You can confirm that we're working with a smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f80ad-13a2-49ca-bbef-495be02c15d5",
   "metadata": {},
   "source": [
    "Compute the average purchase price by brand. We want the results sorted by the average purchase price from the largest to smallest value. As before, round to two digits after the decimal point. This is similar to Q7 above, except _without_ the \"more than 10K\" condition.\n",
    "\n",
    "Implement using the naive **\"version 1\"** algorithm, as described in the lectures:\n",
    "\n",
    "+ You _must_ start with `filtered_events_rdd`.\n",
    "+ You _must_ use `groupByKey()`.\n",
    "+ Per \"version 1\", your implementation _must_ shuffle all values from the \"mappers\" to the \"reducers\".\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f28d7-f55b-454b-9273-3609a0d10f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_5x1 (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "\n",
    "average_revenue_per_brand_v1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221f4d7-4b39-4849-a9f2-d185a26a3209",
   "metadata": {},
   "source": [
    "Compute the average purchase price by brand. We want the results sorted by the average purchase price from the largest to smallest value. As before, round to two digits after the decimal point. This is similar to Q7 above, except _without_ the \"more than 10K\" condition.\n",
    "\n",
    "Implement using the improved **\"version 3\"** algorithm, as described in the lectures:\n",
    "\n",
    "+ You _must_ start with `filtered_events_rdd`.\n",
    "+ You _must_ use `reduceByKey()`.\n",
    "+ Per \"version 3\", your implementation _must_ emit `(sum, count)` pairs and take advantage opportunities to perform aggregations.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024fbcb8-c5bc-4d3c-b22d-f2b99b9b40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_5x2 (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "\n",
    "average_revenue_per_brand_v3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0807e6-179c-4703-af49-a30b00df80fe",
   "metadata": {},
   "source": [
    "## 6. Implementations of Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021be923-60f5-4961-b6fd-f76e48ec759f",
   "metadata": {},
   "source": [
    "Next, we're going to implement joins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88579fa7-cdc4-43d3-a660-ffc09d82e6da",
   "metadata": {},
   "source": [
    "Our join implementations will be general, but we're going to check correctness using the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644786ea-5d01-4db6-a7e9-4f6bd59200ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT * FROM brands b\n",
    "JOIN products p ON p.brand_key = b.brand_key\n",
    "WHERE b.brand_key = '423'\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac59e86",
   "metadata": {},
   "source": [
    "### 6.1 Shuffle Join Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da28f25-363b-4aab-9ab5-8dd091a5f6ee",
   "metadata": {},
   "source": [
    "Here, we're going to implement a shuffle join, aka reduce-side join.\n",
    "\n",
    "Write the function `shuffle_join`, as follows:\n",
    "+ Takes in `R`, `S`, `keyR`, and `keyS`: `R` and `S` are the RDDs to be joined; `keyR` and `keyS` are the join keys in `R` and `S`, respectively (type string).\n",
    "+ The output is an RDD of `Row`s that corresponds to the inner join on the keys.\n",
    "\n",
    "The function should implement a shuffle join between the two RDDs (as discussed in lecture).\n",
    "Specifically:\n",
    "+ You _cannot_ use the `join` (or any related) transformation on RDDs, because that would defeat the point of the exercise.\n",
    "+ If you have any additional questions about allowed or disallowed transformations, ask!\n",
    "\n",
    "Note that in SQL, `keyR` and `keyS` are repeated in the joined output (i.e., you get duplicate columns).\n",
    "Here, you just want one copy.\n",
    "Hint: Concatenate the `Row`s but keep only one copy of the join key.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codecell_61a (keep this id for tracking purposes)\n",
    "\n",
    "def shuffle_join(R, S, keyR, keyS):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6852c41-859a-485f-8300-a37d1ce4b95d",
   "metadata": {},
   "source": [
    "Let's try to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62205616-23f1-46b9-b28e-ff45dfb1bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "shuffle_join_rdd = shuffle_join(brands_rdd, products_rdd, \"brand_key\", \"brand_key\")\n",
    "shuffle_join_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4acd1-197a-4a4e-bdcd-89f7c06e0017",
   "metadata": {},
   "source": [
    "Add in the `WHERE` clause:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17537e-7a50-48b6-a54d-ac3e1bce5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_join_results_rdd = shuffle_join_rdd.filter(lambda row: row[\"brand_key\"] == 423)\n",
    "shuffle_join_results_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1330b-930b-4c05-85df-60453d11b6cb",
   "metadata": {},
   "source": [
    "If you look at the results, they're a bit difficult to read... why don't we just use Spark DataFrames for prettification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bcdc5-cae2-4a36-837d-2a51786d3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(shuffle_join_results_rdd.collect())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4bb60-8cd0-412b-9953-7805492337b9",
   "metadata": {},
   "source": [
    "Verify output against the SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28739c",
   "metadata": {},
   "source": [
    "### 6.2 Replicated Hash Join Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43636b18-7cf2-438c-ade1-1b1a9b180a72",
   "metadata": {},
   "source": [
    "Here, we're going to implement a replicated hash join.\n",
    "\n",
    "Write the function `replicated_hash_join`, as follows:\n",
    "+ Takes in `R`, `S`, `keyR`, and `keyS`: `R` and `S` are the RDDs to be joined; `keyR` and `keyS` are the join keys in `R` and `S`, respectively (type string).\n",
    "+ The output is an RDD of `Row`s that corresponds to the inner join on the keys.\n",
    "\n",
    "The function should implement a hash join between the two RDDs (as discussed in lecture).\n",
    "Specifically:\n",
    "+ `R` is the dataset you load into memory and replicate.\n",
    "+ You _cannot_ use the `join` (or any related) transformation on RDDs, because that would defeat the point of the exercise.\n",
    "+ If you have any additional questions about allowed or disallowed transformations, ask!\n",
    "\n",
    "Note that in SQL, `keyR` and `keyS` are repeated in the joined output (i.e., you get duplicate columns).\n",
    "Here, you just want one copy.\n",
    "Hint: Concatenate the `Row`s but keep only one copy of the join key.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf816e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codecell_62a (keep this id for tracking purposes)\n",
    "\n",
    "def replicated_hash_join(R, S, keyR, keyS):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb139d-b5f6-43cc-97ff-849542f31f71",
   "metadata": {},
   "source": [
    "Let's try to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "replicated_hash_join_rdd = replicated_hash_join(brands_rdd, products_rdd, \"brand_key\", \"brand_key\")\n",
    "replicated_hash_join_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf9916-f31a-4c23-a7e6-f2964c0d5608",
   "metadata": {},
   "source": [
    "Add in the `WHERE` clause:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d216906-91af-4661-bbff-a5d9de5f23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicated_hash_join_results_rdd = replicated_hash_join_rdd.filter(lambda row: row[\"brand_key\"] == 423)\n",
    "replicated_hash_join_results_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd937bb-1a33-4538-a481-a3e75a47fcbd",
   "metadata": {},
   "source": [
    "If you look at the results, they're a bit difficult to read... why don't we just use Spark DataFrames for prettification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c605d94-80a3-4eab-a459-fc3399127373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(replicated_hash_join_results_rdd.collect())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa82c76-7de4-442c-bb7d-5b75d28ebb7e",
   "metadata": {},
   "source": [
    "Verify output against the SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03123495",
   "metadata": {},
   "source": [
    "## 7. Join Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96a587-5ce2-4050-a045-f249ce6bafeb",
   "metadata": {},
   "source": [
    "Now that we have two different implementations of joins, let's compare them, on the _same exact query_.\n",
    "The first two are repeated from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f482150-b30b-4431-90bb-ee28b85bfb33",
   "metadata": {},
   "source": [
    "Let's call this J1 below.\n",
    "(Run the cell, it should just work. If it doesn't you'll need to fix the implementation above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a7ae0-2970-4dfb-87c3-3682e662d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "shuffle_join_rdd = shuffle_join(brands_rdd, products_rdd, \"brand_key\", \"brand_key\").filter(lambda row: row[\"brand_key\"] == 423)\n",
    "shuffle_join_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a8e93-9172-47bd-8fb6-9017086c18b9",
   "metadata": {},
   "source": [
    "Let's call this J2 below.\n",
    "(Run the cell, it should just work. If it doesn't you'll need to fix the implementation above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b76ec2-db1d-4547-a443-9c535bf57d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "replicated_hash_join_rdd = replicated_hash_join(brands_rdd, products_rdd, \"brand_key\", \"brand_key\").filter(lambda row: row[\"brand_key\"] == 423)\n",
    "replicated_hash_join_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436989b3-d611-48aa-864f-de8afe4500ec",
   "metadata": {},
   "source": [
    "Let's call this J3 below.\n",
    "(Run the cell, it should just work. If it doesn't you'll need to fix the implementation above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d156d-9f44-4045-8079-17304d54a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "replicated_hash_join_rdd = replicated_hash_join(products_rdd, brands_rdd, \"brand_key\", \"brand_key\").filter(lambda row: row[\"brand_key\"] == 423)\n",
    "replicated_hash_join_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79bcae4-2b82-45c3-b771-6aa66fa7ee90",
   "metadata": {},
   "source": [
    "J1, J2, and J3 should give you exactly the same results.\n",
    "After all, they're just different implementations of the same query.\n",
    "\n",
    "Answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb03c29-6571-4d38-b971-a953e63c1a65",
   "metadata": {},
   "source": [
    "**Put your answers below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147d654-a741-45b9-bc2f-2d7270656ef1",
   "metadata": {},
   "source": [
    "// qcell_7x1290 (keep this id for tracking purposes)\n",
    "\n",
    "**What are the running times of J1, J2, and J3**?\n",
    "(You might want to run the cells a few times and take the average.)\n",
    "\n",
    "- **Running time of J1:** <font color=\"red\">X.X</font> seconds\n",
    "- **Running time of J2:** <font color=\"red\">X.X</font> seconds\n",
    "- **Running time of J3:** <font color=\"red\">X.X</font> seconds\n",
    "\n",
    "**Explain:**\n",
    "\n",
    "+ If the running times are what you expect, explain why X > Y > Z.\n",
    "+ If the running times are _not_ what you expect, explain what they _should_ be, and then explain why X > Y > Z.\n",
    "+ Specifically compare J2 and J3.\n",
    "\n",
    "**Your answer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ea5ce-0a3c-4ad0-a699-334e14ed251d",
   "metadata": {},
   "source": [
    "# 8. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8db0a8",
   "metadata": {},
   "source": [
    "Details about the Submission of this assignment are outlined in the helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10410d36",
   "metadata": {},
   "source": [
    "## Performance notes\n",
    "- Set and justify `spark.sql.shuffle.partitions` for local vs. cluster runs.\n",
    "- Prefer DataFrame built-ins over Python UDFs; push logic to Catalyst when possible.\n",
    "- Use **AQE** (adaptive query execution) to mitigate skew; consider salting for extreme keys.\n",
    "- Cache only when reuse exists; unpersist when no longer needed.\n",
    "- Use **broadcast join** only when the small side fits in memory; verify with `explain`.\n",
    "- Capture `df.explain(mode='formatted')` for at least one analysis query and one join.\n",
    "- A3 note: Python RDDs cross the Python/JVM boundary; slower runtimes are expected for the RDD parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b6bf9",
   "metadata": {},
   "source": [
    "## Self-check (toy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ea2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark is not None:\n",
    "    a = spark.sparkContext.parallelize([1,2,3,4])\n",
    "    # write some code here to exercise your rdd_mean functions\n",
    "    left = spark.sparkContext.parallelize([(1,'A'), (2,'B'), (3,'C')])\n",
    "    right = spark.sparkContext.parallelize([(1,10), (2,20)])\n",
    "    # write some code here to exercise your join functions\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98307a",
   "metadata": {},
   "source": [
    "## Reproducibility checklist\n",
    "- Record Python, Java, and Spark versions.\n",
    "- Fix timezone to UTC and log run timestamp.\n",
    "- Pin random seeds where randomness is used.\n",
    "- Save configs: `spark.sql.shuffle.partitions`, AQE flags, broadcast thresholds if changed.\n",
    "- Provide exact run commands and input/output paths.\n",
    "- Export a minimal environment file (`environment.yml` or `requirements.txt`).\n",
    "- Keep data paths relative to project root; avoid user-specific absolute paths.\n",
    "- Include small sample outputs for verification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
