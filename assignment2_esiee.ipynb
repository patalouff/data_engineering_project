{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a8e1df",
   "metadata": {},
   "source": [
    "# ESIEE Paris â€” Data Engineering I â€” Assignment 2\n",
    "> Author : Badr TAJINI\n",
    "\n",
    "**Academic year:** 2025â€“2026  \n",
    "**Program:** Data & Applications - Engineering - (FD)   \n",
    "**Course:** Data Engineering I  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13f5da",
   "metadata": {},
   "source": [
    "## Data inputs\n",
    "Define your input paths. Use small CSV/JSON/Parquet files so the notebook runs locally. If your dataset requires credentials, create a **sample subset** and commit only that.\n",
    "\n",
    "**Paths to set:**\n",
    "- `SOURCE_A_PATH` (factâ€‘like dataset)\n",
    "- `SOURCE_B_PATH` (dimensionâ€‘like dataset)\n",
    "- `OUTPUT_BASE` (directory for Parquet output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read carefully the helper to review what is missing here\n",
    "# Example (edit):\n",
    "# SOURCE_A_PATH = 'data/source_a.csv'\n",
    "# SOURCE_B_PATH = 'data/source_b.csv'\n",
    "# OUTPUT_BASE   = 'out/a2_v3_output'\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5ba49",
   "metadata": {},
   "source": [
    "## Pipeline API (implementations required)\n",
    "Implement the following functions. Keep signatures stable. Use explicit schemas when possible. Log counts at each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def ingest(spark, path_a: str, path_b: str) -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"Load SOURCE_A and SOURCE_B. Apply explicit schemas where possible.\n",
    "    Return two DataFrames with uniform column naming.\n",
    "    \"\"\"\n",
    "    # write some code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "def transform(df_a: DataFrame, df_b: DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean, deduplicate, and normalize. Add parsed timestamps.\n",
    "    Drop obvious null records. Prepare keys for join.\n",
    "    \"\"\"\n",
    "    # write some code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "def join_and_aggregate(df: DataFrame, dim: DataFrame) -> DataFrame:\n",
    "    \"\"\"Join with dim table. Handle potential skew (hint: salting or AQE).\n",
    "    Compute business aggregates with window or groupBy.\n",
    "    \"\"\"\n",
    "    # write some code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "def write_out(df: DataFrame, base: str, partitions: list[str]) -> None:\n",
    "    \"\"\"Write Parquet, overwrite mode, partitioned by `partitions`.\n",
    "    Optimize small files if needed (coalesce).\n",
    "    \"\"\"\n",
    "    # write some code here\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdc908",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. **Ingest**: read `SOURCE_A_PATH`, `SOURCE_B_PATH`. Provide explicit schemas. Count rows and malformed records.\n",
    "2. **Transform**: standardize column names, cast types, parse timestamps into UTC, deduplicate using keys.\n",
    "3. **Join + Aggregate**: explain your join strategy. Mitigate skew. Produce a tidy table with daily metrics.\n",
    "4. **Store**: write partitioned Parquet to `OUTPUT_BASE`, e.g., partition by `date` and one categorical column.\n",
    "5. **Explain plans**: capture `df.explain(mode='formatted')` for transform, join, and final write.\n",
    "6. **Quality gates**: implement three checks (row count nonâ€‘zero, null rate thresholds, referential coverage). Abort if a gate fails.\n",
    "7. **Reproducibility**: document your Spark config and any seeds. Describe how to reâ€‘run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af6b4d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Orchestration\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## Replace raises with your implementation, then run this driver.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mspark\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# write some code here\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df_a, df_b \u001b[38;5;241m=\u001b[39m ingest(spark, SOURCE_A_PATH, SOURCE_B_PATH)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# stg = transform(df_a, df_b)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# out = join_and_aggregate(stg, df_b)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print('Final count:', out.count())\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print('Plan:')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# out.explain(mode='formatted')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Orchestration\n",
    "## Replace raises with your implementation, then run this driver.\n",
    "if spark is not None:\n",
    "    # write some code here\n",
    "    # df_a, df_b = ingest(spark, SOURCE_A_PATH, SOURCE_B_PATH)\n",
    "    # stg = transform(df_a, df_b)\n",
    "    # out = join_and_aggregate(stg, df_b)\n",
    "    # print('Final count:', out.count())\n",
    "    # print('Plan:')\n",
    "    # out.explain(mode='formatted')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78262290",
   "metadata": {},
   "source": [
    "# Assignment 2: ETL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e1996-383c-4a4a-b3da-30edcd6e1762",
   "metadata": {},
   "source": [
    "## 1. Querying the Operational Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ab400",
   "metadata": {},
   "source": [
    "Let's run a query to verify that the operational database has been properly restored and that we can issue a query to PostgreSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a33f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: psql: command not found\n",
      "sudo: unknown user postgres\n",
      "sudo: error initializing audit plugin sudoers_audit\n"
     ]
    }
   ],
   "source": [
    "#First cell won't run\n",
    "!PGPORT=5433 psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT user_id) AS number_users FROM retail.user;\"\n",
    "\n",
    "#Solution 1\n",
    "!sudo -u postgres env PGPORT=5433 psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT user_id) AS number_users FROM retail.user;\"\n",
    "\n",
    "# Solution 2\n",
    "# 1- Run this in your terminal to create a read-only user for the database :\n",
    "# sudo -u postgres env PGPORT=5433 psql -d esiee_full -v ON_ERROR_STOP=1 -c \\\n",
    "# \"CREATE ROLE esiee_reader LOGIN PASSWORD 'azerty123';\n",
    "# GRANT CONNECT ON DATABASE esiee_full TO esiee_reader;\n",
    "# GRANT USAGE ON SCHEMA retail TO esiee_reader;\n",
    "# GRANT SELECT ON ALL TABLES IN SCHEMA retail TO esiee_reader;\n",
    "# ALTER DEFAULT PRIVILEGES IN SCHEMA retail GRANT SELECT ON TABLES TO esiee_reader;\"\n",
    "# 2- Then, in your notebook, set the environment variables like this:\n",
    "# import os\n",
    "# os.environ['PGHOST'] = '127.0.0.1'   # force TCP (not Unix socket)\n",
    "# os.environ['PGPORT'] = '5433'\n",
    "# os.environ['PGUSER'] = 'esiee_reader'\n",
    "# os.environ['PGPASSWORD'] = 'azerty123'\n",
    "# 3- then\n",
    "# !psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT COUNT(DISTINCT user_id) AS number_users FROM retail.user;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851c65a-e658-4044-8878-4c3d3722463e",
   "metadata": {},
   "source": [
    "**The correct answer should be 3022290.**\n",
    "\n",
    "If running the cell above gives you the same answer, the everything should be in order.\n",
    "\n",
    "If you're getting an error, fix it before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d489b61-7304-4513-8c79-89c07745a3ec",
   "metadata": {},
   "source": [
    "As a warmup exercise, write SQL queries against the operational database to answer the following questions and report the answers.\n",
    "Place both your SQL queries and answers in the following cell, replacing the placeholder texts that exist there.\n",
    "Each question needs to be answered by a _single_ SQL query (that is, it is not acceptable to run multiple SQL queries and then compute the answer yourself).\n",
    "\n",
    "1. For `session_id` `789d3699-028e-4367-b515-b82e2cb5225f`, what was the purchase price?\n",
    "2. How many products are sold by the brand \"sokolov\"?\n",
    "3. What is the average purchase price of items purchased from the brand \"febest\"?\n",
    "4. What is average number of events per user? (Report answer to two digits after the decimal point, i.e., XX.XX)\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b515132-07e2-44e3-abea-287b0a89af4a",
   "metadata": {},
   "source": [
    "// qcell_1b76x2 (keep this id for tracking purposes)\n",
    "\n",
    "**Q1 SQL:**\n",
    "\n",
    "SELECT....\n",
    "\n",
    "**Q1 answer:**\n",
    "\n",
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT price \\\n",
    "FROM retail.events \\\n",
    "WHERE session_id = '789d3699-028e-4367-b515-b82e2cb5225f' \\\n",
    "AND event_type = 'purchase';\"\n",
    "\n",
    "**Q2 SQL:**\n",
    "\n",
    "SELECT....\n",
    "\n",
    "**Q2 answer:**\n",
    "\n",
    "XXXX\n",
    "\n",
    "**Q3 SQL:**\n",
    "\n",
    "SELECT....\n",
    "\n",
    "**Q3 answer:**\n",
    "\n",
    "XXXX\n",
    "\n",
    "**Q4 SQL:**\n",
    "\n",
    "SELECT....\n",
    "\n",
    "**Q4 answer:**\n",
    "\n",
    "XXXX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f486e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 answer :\n",
    "\n",
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT price \\\n",
    "FROM retail.events \\\n",
    "WHERE session_id = '789d3699-028e-4367-b515-b82e2cb5225f' \\\n",
    "AND event_type = 'purchase';\"\n",
    "\n",
    "# or\n",
    "\n",
    "!psql \"esiee_full\" -v ON_ERROR_STOP=1 -c \"SELECT * \\\n",
    "FROM retail.events \\\n",
    "WHERE session_id = '789d3699-028e-4367-b515-b82e2cb5225f' \\\n",
    "AND event_type = 'purchase';\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fecda4-fc6e-4767-af8f-c08d72eeaa02",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec996e",
   "metadata": {},
   "source": [
    "The following cell contains setup to measure wall clock time and memory usage. (Don't worry about the details, just run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbf5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy pandas pyarrow matplotlib scipy\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import psutil  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "print(\"psutil is installed.\")\n",
    "\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import time, os, platform\n",
    "\n",
    "# Try to import optional modules\n",
    "try:\n",
    "    import psutil\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "try:\n",
    "    import resource  # not available on Windows\n",
    "except Exception:\n",
    "    resource = None\n",
    "\n",
    "\n",
    "def _rss_bytes():\n",
    "    \"\"\"Resident Set Size in bytes (cross-platform via psutil if available).\"\"\"\n",
    "    if psutil is not None:\n",
    "        return psutil.Process(os.getpid()).memory_info().rss\n",
    "    # Fallback: unknown RSS â†’ 0 \n",
    "    return 0\n",
    "\n",
    "\n",
    "def _peak_bytes():\n",
    "    \"\"\"\n",
    "    Best-effort peak memory in bytes.\n",
    "    - Windows: psutil peak working set (peak_wset)\n",
    "    - Linux:   resource.ru_maxrss (KB â†’ bytes)\n",
    "    - macOS:   resource.ru_maxrss (bytes)\n",
    "    Fallback to current RSS if unavailable.\n",
    "    \"\"\"\n",
    "    sysname = platform.system()\n",
    "\n",
    "    # Windows path: use psutil peak_wset if present\n",
    "    if sysname == \"Windows\" and psutil is not None:\n",
    "        mi = psutil.Process(os.getpid()).memory_info()\n",
    "        peak = getattr(mi, \"peak_wset\", None)  # should be available on Windows\n",
    "        if peak is not None:\n",
    "            return int(peak)\n",
    "        return int(mi.rss)\n",
    "\n",
    "    # POSIX path: resource may be available\n",
    "    if resource is not None:\n",
    "        try:\n",
    "            ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "            # On Linux ru_maxrss is in kilobytes; on macOS/BSD it is bytes\n",
    "            if sysname == \"Linux\":\n",
    "                return int(ru) * 1024\n",
    "            else:\n",
    "                return int(ru)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Last resort\n",
    "    return _rss_bytes()\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def timemem(line, cell):\n",
    "    \"\"\"\n",
    "    Measure wall time and memory around the execution of this cell.\n",
    "\n",
    "        %%timemem\n",
    "        <your code>\n",
    "\n",
    "    Notes:\n",
    "    - RSS = resident memory after the cell.\n",
    "    - Peak is OS-dependent (see _peak_bytes docstring).\n",
    "    \"\"\"\n",
    "    ip = get_ipython()\n",
    "\n",
    "    rss_before  = _rss_bytes()\n",
    "    peak_before = _peak_bytes()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Execute the cell body\n",
    "    result = ip.run_cell(cell)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    rss_after  = _rss_bytes()\n",
    "    peak_after = _peak_bytes()\n",
    "\n",
    "    wall = t1 - t0\n",
    "    rss_delta_mb  = (rss_after  - rss_before)  / (1024 * 1024)\n",
    "    peak_delta_mb = (peak_after - peak_before) / (1024 * 1024)\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"Wall time: {wall:.3f} s\")\n",
    "    print(f\"RSS Î”: {rss_delta_mb:+.2f} MB\")\n",
    "    print(f\"Peak memory Î”: {peak_delta_mb:+.2f} MB (OS-dependent)\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ca1f2-5c25-49df-9570-f5e56782b55c",
   "metadata": {},
   "source": [
    "## 3. The \"Extract\" in ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb0763",
   "metadata": {},
   "source": [
    "The operational database comprises the tables described in the helper.\n",
    "\n",
    "For the \"Extract\" in ETL, we're going to extract the following CSV files, each corresponding to a table in the operational database:\n",
    "\n",
    "- **user.csv**: `user_id, gender, birthdate`\n",
    "- **session.csv**: `session_id, user_id`\n",
    "- **product.csv**: `product_id, brand, category, product_name`\n",
    "- **product_name.csv**: `category, product_name, description`\n",
    "- **events.csv**: `event_time, event_type, session_id, product_id, price`\n",
    "- **category.csv**: `category, description`\n",
    "- **brand.csv**: `brand, description`\n",
    "\n",
    "From these files, we'll build a data warehouse organized in a standard star schema that has the following tables:\n",
    "\n",
    "- Dimension tables: `dim_user`, `dim_age`, `dim_brand`, `dim_category`, `dim_product`, `dim_date`, `dim_session`\n",
    "- The main fact table `fact_events` with foreign keys: `date_key, user_key, age_key, product_key, brand_key, category_key, session_key`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d94b96-231a-4db7-9c60-5ae521a38aae",
   "metadata": {},
   "source": [
    "Let's specify a \"base directory\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd673d3-8c30-489a-822c-27419c1e1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to path on your local machine.\n",
    "BASE_DIR = \"/Users/btajini/de1-work/assignment2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25b07b-0a28-499a-9817-0630640aea20",
   "metadata": {},
   "source": [
    "These are the commands that perform the \"extraction\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"user\"         TO '\\''{BASE_DIR}/user.csv'\\''         WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"session\"      TO '\\''{BASE_DIR}/session.csv'\\''      WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"category\"     TO '\\''{BASE_DIR}/category.csv'\\''     WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"brand\"        TO '\\''{BASE_DIR}/brand.csv'\\''        WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"product_name\" TO '\\''{BASE_DIR}/product_name.csv'\\'' WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"product\"      TO '\\''{BASE_DIR}/product.csv'\\''      WITH (FORMAT csv, HEADER true)'\n",
    "!psql cs451 -v ON_ERROR_STOP=1 -c '\\copy \"retail\".\"events\"       TO '\\''{BASE_DIR}/events.csv'\\''       WITH (FORMAT csv, HEADER true)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f0dde-6c17-48ad-aa4e-6e4019b106bc",
   "metadata": {},
   "source": [
    "(Note that the quote style above will _not_ work for Windows machines. Please adjust accordingly.)\n",
    "\n",
    "After the extraction, you should have 7 CSV files, each corresponding to a table in the operational database.\n",
    "\n",
    "The CSV files should be stored in `BASE_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930cdf5-b989-4608-8a03-22c0eab6b031",
   "metadata": {},
   "source": [
    "The following code snippet should \"just work\" to initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe303ff-e323-4389-a64f-d9e730fa1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark, os\n",
    "\n",
    "# Change to path on your local machine.\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/jimmylin/Dropbox/workspace/teaching/spark-4.0.0-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "py = sys.executable  # the Python of this notebook (e.g., .../envs/yourenv/bin/python)\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = py\n",
    "os.environ[\"PYSPARK_PYTHON\"] = py\n",
    "\n",
    "spark = SparkSession.getActiveSession() or (\n",
    "    SparkSession.builder\n",
    "    .appName(\"A2\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")           # or 12g+\n",
    "    .config(\"spark.sql.shuffle.partitions\",\"400\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.pyspark.driver.python\", py)\n",
    "    .config(\"spark.pyspark.python\", py)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", py)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec616b-b333-4eb6-8d7e-ab002af8f668",
   "metadata": {},
   "source": [
    "At this point, Spark should be initialized.\n",
    "\n",
    "Let's then load in CSV files into DataFrames.\n",
    "\n",
    "write some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49534dee-f56a-4f6b-8930-64092a0fcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_30z8le (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, we've loaded each of the CSV files into a corresponding dataframe.\n",
    "# Let's count the number of records in each:\n",
    "\n",
    "print(f\"user: {df_user.count()}\")\n",
    "print(f\"session: {df_session.count()}\")\n",
    "print(f\"product: {df_product.count()}\")\n",
    "print(f\"product_name: {df_product_name.count()}\")\n",
    "print(f\"events: {df_events.count()}\")\n",
    "print(f\"category: {df_category.count()}\")\n",
    "print(f\"brand: {df_brand.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552f156-6f37-4938-8e93-1a02a38ba760",
   "metadata": {},
   "source": [
    "How do you know if you've done everything correctly?\n",
    "\n",
    "Well, issue the SQL query `select count(*) from retail.user;` to count the number of rows in the `user` table in the operational database.\n",
    "It should match the output of `df_user.count()`; same for the other tables.\n",
    "If the counts match, then you know everything is in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e10f63",
   "metadata": {},
   "source": [
    "## 4. Build the Dimensions Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3303f4-e9be-455b-a458-91de0ebdded7",
   "metadata": {},
   "source": [
    "### 4.1 The `user` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4ee83",
   "metadata": {},
   "source": [
    "Build the `dim_user` dimension table.\n",
    "This table should include `user_key`, `user_id`, `gender`, `birthdate`, and `generation`. \n",
    "\n",
    "Set `generation` to one of the following values based on the birth year: \n",
    "- \"Traditionalists\": born 1925 to 1945\n",
    "- \"Boomers\": born 1946 to 1964\n",
    "- \"GenX\": born 1965 to 1980\n",
    "- \"Millennials\": born 1981 to 2000\n",
    "- \"GenZ\": born 2001 to 2020\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883cc05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%timemem` not found.\n"
     ]
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_41ax14 (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"dim_user\" should hold the user dimensions table according to the specification above.\n",
    "\n",
    "dim_user.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6e753-8515-4924-b4d7-f4a8eeb3c4f8",
   "metadata": {},
   "source": [
    "**The correct answer should be 3022290.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eadced-3481-480a-9320-a07bbfc6d6b7",
   "metadata": {},
   "source": [
    "### 4.2 The `age` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758f1e8",
   "metadata": {},
   "source": [
    "Even though `birthdate` exists in `dim_user`, a separate `dim_age` is helpful because it:\n",
    "- Simplifies analysis with ready-made bands.\n",
    "- Ensures consistency across all queries.\n",
    "- Improves performance via small surrogate keys.\n",
    "- Preserves history by fixing age at event time.\n",
    "- Adds flexibility to adjust bands without changing facts.\n",
    "\n",
    "We're going to build a `dim_age` table that has 4 columns:\n",
    "- `age_key`: (INT, surrogate PK)\n",
    "- `age_band`: (STRING) following the age band rules below\n",
    "- `min_age`: (INT)\n",
    "- `max_age`: (INT)\n",
    "\n",
    "Bands:\n",
    "- \"<18\": min_age = NULL, max_age = 17\n",
    "- \"18-24\": 18, 24\n",
    "- \"25-34\": 25, 34\n",
    "- \"35-44\": 35, 44\n",
    "- \"45-54\": 45, 54\n",
    "- \"55-64\": 55, 64\n",
    "- \"65-74\": 65, 74\n",
    "- \"75-84\": 75, 84\n",
    "- \"85-94\": 85, 94\n",
    "- \"unknown\": NULL, NULL\n",
    "\n",
    "The construction of this table is a bit tricky, so we're going to show you how to do it, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed16029",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "# Static age bands\n",
    "age_band_rows = [\n",
    "    (\"<18\",   None, 17),\n",
    "    (\"18-24\", 18, 24),\n",
    "    (\"25-34\", 25, 34),\n",
    "    (\"35-44\", 35, 44),\n",
    "    (\"45-54\", 45, 54),\n",
    "    (\"55-64\", 55, 64),\n",
    "    (\"65-74\", 65, 74),\n",
    "    (\"75-84\", 75, 84),\n",
    "    (\"85-94\", 85, 94),\n",
    "    (\"unknown\", None, None),\n",
    "]\n",
    "dim_age = spark.createDataFrame(age_band_rows, [\"age_band\", \"min_age\", \"max_age\"])\n",
    "\n",
    "w_age = Window.orderBy(F.col(\"age_band\"))\n",
    "dim_age = dim_age.withColumn(\"age_key\", F.dense_rank().over(w_age))\n",
    "\n",
    "dim_age.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55857d00-560c-4beb-8d29-8626369a3110",
   "metadata": {},
   "source": [
    "**The correct answer should be 10.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012e1ca-e6ca-46ae-a873-7f5771cd4310",
   "metadata": {},
   "source": [
    "### 4.3 The `brand`, `product`, and `category` Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe029f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Build the following dimension tables:\n",
    "\n",
    "**`dim_brand`:**\n",
    "- `brand_key` (INT, surrogate PK)\n",
    "- `brand_code` (STRING) \n",
    "- `brand_desc` (STRING)\n",
    "\n",
    "**`dim_category`:**\n",
    "- `category_key` (INT, surrogate PK)\n",
    "- `category_code` (STRING) \n",
    "- `category_desc` (STRING)\n",
    "\n",
    "**`dim_product`:**\n",
    "- `product_key`  (INT, surrogate PK)\n",
    "- `product_id`   (STRING)\n",
    "- `product_desc` (STRING)\n",
    "- `brand_key`   (INT, FK â†’ `dim_brand`)  \n",
    "- `category_key`(INT, FK â†’ `dim_category`)\n",
    "\n",
    "The Learning goals of `dim_product` is to keep all products in `product`, and add details from `product_names`, then join the results with `brand` and `category` dimension tables.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acddfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_43k3n9 (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"dim_brand\", \"dim_category\", and \"dim_product\" should hold \n",
    "# the dimension tables according to the specifications above.\n",
    "\n",
    "print(f\"Number of rows in dim_brand: {dim_brand.count()}\")\n",
    "print(f\"Number of rows in dim_category: {dim_category.count()}\")\n",
    "print(f\"Number of rows in dim_product: {dim_product.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b819a9-6b4c-4a1f-82a3-e379d1d6a2ad",
   "metadata": {},
   "source": [
    "**Correct answers:**\n",
    "\n",
    "+ Number of rows in `dim_brand`: 3444\n",
    "+ Number of rows in `dim_category`: 13\n",
    "+ Number of rows in `dim_product`: 166794"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db6b6a-b5af-46ac-ab1c-161cf50ea75a",
   "metadata": {},
   "source": [
    "### 4.4  The `date` Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f27d5",
   "metadata": {},
   "source": [
    "This table is expected to have one row per calendar date. \n",
    "\n",
    "**`dim_date`:**\n",
    "- `date_key`     (INT, surrogate PK; format YYYYMMDD)\n",
    "- `date`         (DATE, the actual calendar date)\n",
    "- `day`          (INT, 1â€“31)\n",
    "- `day_of_week`  (INT, 1=Mon â€¦ 7=Sun)\n",
    "- `day_name`     (STRING, e.g., Monday)\n",
    "- `is_weekend`   (BOOLEAN)\n",
    "- `week_of_year` (INT, 1â€“53, ISO week)\n",
    "- `month`        (INT, 1â€“12)\n",
    "- `month_name`   (STRING, e.g., January)\n",
    "- `quarter`      (INT, 1â€“4)\n",
    "- `year`         (INT)\n",
    "\n",
    "\n",
    "There are 2025 years, each with 365 days. Do we need to have a table that big? \n",
    "We can, but we do not have to! \n",
    "\n",
    "Instead, follow these instructions to create only as many rows as we need:\n",
    "\n",
    "1. Determine the date range (from the min and max `event_date` in `df_events`).\n",
    "2. Generate all dates in that range with `F.sequence()`.\n",
    "3. Derive attributes (`day`, `day_of_week`, ...).\n",
    "4. Create `date_key` = `year * 10000 + month * 100 + day` (i.e., YYYYMMDD).\n",
    "5. Assign `date_key` as the surrogate PK.\n",
    "\n",
    "Build the `dim_date` table conforming to the specifications above.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417d0554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%timemem` not found.\n"
     ]
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_44qm5c (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"dim_date\" should hold the dates dimension table according to the specification above.\n",
    "\n",
    "print(dim_date.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191b0ce-16fe-4359-bdc2-b23030c5e4f0",
   "metadata": {},
   "source": [
    "**The correct answer should be 32.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e838229",
   "metadata": {},
   "source": [
    "If you reach here, congratulations!\n",
    "You have created all the dimension tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "\n",
    "print(f\"dim_user: {dim_user.count()}\")\n",
    "print(f\"dim_age: {dim_age.count()}\")\n",
    "print(f\"dim_brand: {dim_brand.count()}\")\n",
    "print(f\"dim_category: {dim_category.count()}\")\n",
    "print(f\"dim_product: {dim_product.count()}\")\n",
    "print(f\"dim_date: {dim_date.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991409d-c410-46b3-bf2c-b3acb4b5fb28",
   "metadata": {},
   "source": [
    "**Correct answers:**\n",
    "\n",
    "- `dim_user`: 3022290\n",
    "- `dim_age`: 10\n",
    "- `dim_brand`: 3444\n",
    "- `dim_category`: 13\n",
    "- `dim_product`: 166794\n",
    "- `dim_date`: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fa63f-101b-444d-9013-20c2947657b4",
   "metadata": {},
   "source": [
    "## 5. Build the Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3523d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now it's time to build the fact table!\n",
    "\n",
    "Our goal in this step is to create a clean `fact_events` table that joins the events from the operational database to the dimension tables you've just built above.\n",
    "Along the way, we're going to enforce data quality and do a bit of data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb231a1-f368-40b1-aa3c-e448c151bbec",
   "metadata": {},
   "source": [
    "### 5.1 Clean Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75d9cc-9eb4-47e5-85dd-f073172fc81c",
   "metadata": {},
   "source": [
    "Create `events_clean` by removing any record that \"does not make sense\".\n",
    "Specifically:\n",
    "\n",
    "- Start from the `df_events` DataFrame.\n",
    "- Keep only rows with non-null timestamps, `session_id`, and `product_id`.\n",
    "- Cast price to double; keep `NULL` prices (views/carts can be price-less) and non-negative values only.\n",
    "- Drop dates in the future.\n",
    "- Restrict to valid event types: `view`, `cart`, `purchase`, `remove`.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ae17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_51ep7v (keep this id for tracking purposes)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from operator import and_ as AND\n",
    "\n",
    "valid_types = [\"view\", \"cart\", \"purchase\", \"remove\"]\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"events_clean\" should conform to the specification above.\n",
    "\n",
    "events_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fd335",
   "metadata": {},
   "source": [
    "### 5.2 Cap Silly Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6f0df-6101-4c96-a30a-03b498844a20",
   "metadata": {},
   "source": [
    "Next, let us check some statistics about prices and then decide what we want to do.\n",
    "\n",
    "What is the minimum, maximum, and average price in this database?\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_52hg6x (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"minimum\", \"maximum\", and \"average\" should conform to the specification above.\n",
    "\n",
    "print(f\"minimum: {minimum}\")\n",
    "print(f\"maximum: {maximum}\")\n",
    "print(f\"average: {average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b48511",
   "metadata": {},
   "source": [
    "Wait, something's not right! \n",
    "The average price is 864.27 but the maximum seems suss...\n",
    "It is possible these high prices are just errors.\n",
    "\n",
    "For simplicity, let us assume a threshold value equal to 100x the average, and remove anything more than that.\n",
    "Filter `events_clean` as described.\n",
    "\n",
    "write some code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9783cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%timemem` not found.\n"
     ]
    }
   ],
   "source": [
    "%%timemem\n",
    "# codecell_52bf5d (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"events_clean\" should conform to the specification above.\n",
    "\n",
    "events_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33e64b",
   "metadata": {},
   "source": [
    "Good, we still have about 42.4M records, but we've done some basic data cleaning.\n",
    "Let us continue..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3065688-4d49-4567-b7f5-eb866bf52441",
   "metadata": {},
   "source": [
    "### 5.3 Build Tiny Lookup Tables (LKPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5cb09-baf1-45c4-9b6b-0083c1d86b4b",
   "metadata": {},
   "source": [
    "Create lookup tables that help us connect `events_clean` with the dimension tables we created:\n",
    "\n",
    "- `user_lkp`: (`user_id` â†’ `user_key`) from `dim_user`.\n",
    "- `prod_lkp`: (`product_id` â†’ `product_key`, `brand_key`, `category_key`) from `dim_product`.\n",
    "- `date_lkp`: (`date` â†’ `date_key`) from `dim_date`.\n",
    "- session-to-user bridge: use the raw `df_session` (`session_id`, `user_id`) CSV (not a dimension) to pull `user_id`.\n",
    "\n",
    "**Hint:** These LKPs are just calling `select` from the right sources with the right parameters.\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056954dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_53l2kp (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, the following variables should conform to the specification above.\n",
    "\n",
    "print(session_bridge.count(), user_lkp.count(), prod_lkp.count(), date_lkp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b6b4e-0449-4db7-8971-98ceabe29eb9",
   "metadata": {},
   "source": [
    "### 5.4 Join Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad90901",
   "metadata": {},
   "source": [
    "Finally, join everything together to create `fact_events`.\n",
    "Follow the following steps:\n",
    "\n",
    "- Start from `clean events` with these columns: (`event_time`, `event_type`, `session_id`, `product_id`, `price`, `date`).\n",
    "- Join sessions first (to get `user_id`).\n",
    "- Then join product, date, and user.\n",
    "- Join with `dim_user` to find out the birthdate and compute user age at the day of the event in `age_on_event`.\n",
    "- Join with `dim_age` to find the age band based on `age_on_event`.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You built the LKPs for a reason... use them.\n",
    "- Left, right, or natural joins?\n",
    "\n",
    "The final part above is a bit tricky, so we'll just give you the answer. But you'll need to figure out how it integrates with everything above.\n",
    "\n",
    "```\n",
    "        .withColumn(\"age_on_event\", F.floor(F.months_between(F.col(\"date\"), F.to_date(\"birthdate\"))/12))\n",
    "        .join(\n",
    "           dim_age.select(\"age_key\", \"age_band\", \"min_age\", \"max_age\"),\n",
    "           (\n",
    "               ((F.col(\"age_on_event\") > F.col(\"min_age\"))) &\n",
    "               ((F.col(\"age_on_event\") <= F.col(\"max_age\")))\n",
    "           ),\n",
    "           \"left\"\n",
    "       )\n",
    "```\n",
    "\n",
    "The final result (`fact_events`) should include the following columns:\n",
    "\n",
    "- `date_key`\n",
    "- `user_key`\n",
    "- `age_key`\n",
    "- `product_key`\n",
    "- `brand_key`\n",
    "- `category_key`\n",
    "- `session_id`\n",
    "- `event_time`\n",
    "- `event_type`\n",
    "- `price`\n",
    "\n",
    "**write some code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "# codecell_54aaaa (keep this id for tracking purposes)\n",
    "\n",
    "# TODO: Write your code below, but do not remove any lines already in this cell.\n",
    "\n",
    "\n",
    "# By the time we get to here, \"fact_events\" should conform to the specification above.\n",
    "\n",
    "print(fact_events.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdfc6c",
   "metadata": {},
   "source": [
    "Congrats, you've done it!\n",
    "You've created the fact table successfuly! ðŸš€\n",
    "\n",
    "Here is the summary of the schema:\n",
    "\n",
    "- `date_key` (FK â†’ `dim_date`)\n",
    "- `user_key` (FK â†’ `dim_user`)\n",
    "- `age_key`  (FK â†’ `dim_age`)\n",
    "- `product_key` (FK â†’ `dim_product`)\n",
    "- `brand_key` (FK â†’ `dim_brand`)\n",
    "- `category_key` (FK â†’ `dim_category`)\n",
    "- `session_id` (STRING, business key, kept directly in this table)\n",
    "- `event_time` (TIMESTAMP)\n",
    "- `event_tpe` (STRING)\n",
    "- `price` (DOUBLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa5c0a-b7ab-4340-9b05-1dc615444254",
   "metadata": {},
   "source": [
    "## 6. Export the Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebb4ea",
   "metadata": {},
   "source": [
    "You now have a shiny `fact_events` table!\n",
    "But how should you store it?\n",
    "(Remember our discussion in class about row vs. column representations?)\n",
    "\n",
    "Let's store `fact_events` in a few different ways and compare data sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb22a99-f504-4487-97c3-324554333091",
   "metadata": {},
   "source": [
    "First, let's try writing out as CSV files, both compressed and uncompressed, per below.\n",
    "\n",
    "Note that in Spark, we specify the output _directory_, which is then populated with many \"part\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42721a-f950-411d-93ce-2069629f019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_events.write.mode(\"overwrite\").option(\"header\", True).csv(BASE_DIR + \"/fact_events.csv\")\n",
    "fact_events.write.mode(\"overwrite\").option(\"header\", True).option(\"compression\", \"snappy\").csv(BASE_DIR + \"/fact_events.csv.snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb2541-f412-437f-8b1f-d973ab722ab5",
   "metadata": {},
   "source": [
    "Let's then try Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9721b6-d435-4438-bf06-131a8d0a4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_events.write.mode(\"overwrite\").parquet(BASE_DIR + \"/fact_events.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d8161-114a-4896-b5b9-aa7a27c02e76",
   "metadata": {},
   "source": [
    "Let's compare the output sizes using the following bit of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bd520-4a21-4161-b6c2-6758bb6546e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for f in [BASE_DIR + \"/fact_events.csv\", BASE_DIR + \"/fact_events.csv.snappy\", BASE_DIR + \"/fact_events.parquet\"]:\n",
    "    try:\n",
    "        size = sum(os.path.getsize(os.path.join(dp, fn))\n",
    "                   for dp, dn, filenames in os.walk(f)\n",
    "                   for fn in filenames)\n",
    "        print(f\"{f}: {size/(1024*1024*1024):.1f} GB\")\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d796fa9-cad5-4ca6-b6cd-cbae63cdc96e",
   "metadata": {},
   "source": [
    "**your answers below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56df9e6-a6bd-4ebd-a692-63c4bd590475",
   "metadata": {},
   "source": [
    "// qcell_6a9876 (keep this id for tracking purposes)\n",
    "\n",
    "- **Size of CSV output, no compression:** XX  GB\n",
    "- **Size of CSV output, Snappy compression:** XX GB\n",
    "- **Size of Parquet output:** XX GB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b7369-6ce3-455a-917b-bc002e112da4",
   "metadata": {},
   "source": [
    "**Answer the following question:**\n",
    "\n",
    "Q6.1 Why is columnar storage (Parquet) usually much smaller?\n",
    "\n",
    "Q6.2 Which format is better for analytical queries and why?\n",
    "\n",
    "**your answers below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1b29b-375e-4143-b844-cd40c3183e0f",
   "metadata": {},
   "source": [
    "// qcell_6b1234 (keep this id for tracking purposes)\n",
    "\n",
    "**Q6.1 Answer:**\n",
    "\n",
    "**your answer!**\n",
    "\n",
    "**Q6.2 Answer:**\n",
    "\n",
    "**your answer!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5ab3d-73dd-4bd9-9722-4f07e6b913c0",
   "metadata": {},
   "source": [
    "## 7. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bb473",
   "metadata": {},
   "source": [
    "Details about the Submission of this assignment are outlined in the helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89841125",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timemem\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95cb45",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- This notebook with all code cells executed.\n",
    "- A brief `REPORT.md` with: inputs, assumptions, plan screenshots, quality results, and performance choices.\n",
    "- Output folder with Parquet sample (â‰¤20 MB).\n",
    "\n",
    "## Evaluation\n",
    "- Correctness and clarity of pipeline (40%).\n",
    "- Dataâ€‘quality gates and rationale (20%).\n",
    "- Performance reasoning and plan analysis (20%).\n",
    "- Reproducibility and organization (20%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46acfb1",
   "metadata": {},
   "source": [
    "## Performance notes\n",
    "- Record `spark.sql.shuffle.partitions` and justify your value.\n",
    "- Show one example of avoiding UDFs by using builtâ€‘ins.\n",
    "- If you use broadcast join, explain why it is safe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5583ac",
   "metadata": {},
   "source": [
    "## Reproducibility checklist\n",
    "- List Spark version and key configs.\n",
    "- Fix time zone to UTC.\n",
    "- Control randomness if used.\n",
    "- Provide exact commands to run the notebook endâ€‘toâ€‘end.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
