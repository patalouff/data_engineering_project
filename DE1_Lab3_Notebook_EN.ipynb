{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f39071",
   "metadata": {},
   "source": [
    "# DE1 — Lab 3: Physical Representations and Batch II Costs\n",
    "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
    "---\n",
    "\n",
    "Execute all cells. Capture plans and Spark UI evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785771b",
   "metadata": {},
   "source": [
    "## 0. Setup and explicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "spark = SparkSession.builder.appName(\"de1-lab3\").getOrCreate()\n",
    "\n",
    "clicks_schema = T.StructType([\n",
    "    T.StructField(\"prev_title\", T.StringType(), True),\n",
    "    T.StructField(\"curr_title\", T.StringType(), True),\n",
    "    T.StructField(\"type\", T.StringType(), True),\n",
    "    T.StructField(\"n\", T.IntegerType(), True),\n",
    "    T.StructField(\"ts\", T.TimestampType(), True),\n",
    "])\n",
    "dim_schema = T.StructType([\n",
    "    T.StructField(\"curr_title\", T.StringType(), True),\n",
    "    T.StructField(\"curr_category\", T.StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee689bfd",
   "metadata": {},
   "source": [
    "## 1. Ingest monthly CSVs (row format baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98dbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"data/\"\n",
    "paths = [f\"{base}lab3_clicks_2025-05.csv\", f\"{base}lab3_clicks_2025-06.csv\", f\"{base}lab3_clicks_2025-07.csv\"]\n",
    "row_df = (spark.read.schema(clicks_schema).option(\"header\",\"true\").csv(paths)\n",
    "            .withColumn(\"year\", F.year(\"ts\")).withColumn(\"month\", F.month(\"ts\")))\n",
    "row_df.cache()\n",
    "print(\"Rows:\", row_df.count())\n",
    "row_df.printSchema()\n",
    "row_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04c2ce",
   "metadata": {},
   "source": [
    "### Evidence: row representation plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Q1: top transitions per month for 'link'\n",
    "q1_row = (row_df.filter(F.col(\"type\")==\"link\")\n",
    "           .groupBy(\"year\",\"month\",\"prev_title\",\"curr_title\")\n",
    "           .agg(F.sum(\"n\").alias(\"n\"))\n",
    "           .orderBy(F.desc(\"n\"))\n",
    "           .limit(50))\n",
    "q1_row.explain(\"formatted\")\n",
    "\n",
    "import pathlib, datetime as _dt\n",
    "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
    "with open(\"proof/plan_row.txt\",\"w\") as f:\n",
    "    f.write(str(_dt.datetime.now())+\"\\n\")\n",
    "    f.write(q1_row._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_row.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c150aa4",
   "metadata": {},
   "source": [
    "## 2. Column representation: Parquet with partitioning and optional sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_base = \"outputs/lab3/columnar\"\n",
    "# Write columnar\n",
    "(row_df\n",
    " .write.mode(\"overwrite\")\n",
    " .partitionBy(\"year\",\"month\")\n",
    " .parquet(f\"{col_base}/clicks_parquet\"))\n",
    "\n",
    "# Re‑read columnar for fair comparison\n",
    "col_df = spark.read.schema(clicks_schema.add(\"year\",\"int\").add(\"month\",\"int\")).parquet(f\"{col_base}/clicks_parquet\")\n",
    "col_df.cache()\n",
    "print(\"Columnar rows:\", col_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d494c84",
   "metadata": {},
   "source": [
    "### Evidence: column representation plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_col = (col_df.filter(F.col(\"type\")==\"link\")\n",
    "           .groupBy(\"year\",\"month\",\"prev_title\",\"curr_title\")\n",
    "           .agg(F.sum(\"n\").alias(\"n\"))\n",
    "           .orderBy(F.desc(\"n\"))\n",
    "           .limit(50))\n",
    "q1_col.explain(\"formatted\")\n",
    "with open(\"proof/plan_column.txt\",\"w\") as f:\n",
    "    from datetime import datetime as _dt\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(q1_col._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_column.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b040d812",
   "metadata": {},
   "source": [
    "## 3. Join strategy: normal vs broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2787de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = spark.read.schema(dim_schema).option(\"header\",\"true\").csv(\"data/lab3_dim_curr_category.csv\")\n",
    "# Non‑broadcast join\n",
    "j1 = (col_df.join(dim, \"curr_title\", \"left\")\n",
    "      .groupBy(\"curr_category\")\n",
    "      .agg(F.sum(\"n\").alias(\"total_n\"))\n",
    "      .orderBy(F.desc(\"total_n\")))\n",
    "j1.explain(\"formatted\")\n",
    "\n",
    "# Broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "j2 = (col_df.join(broadcast(dim), \"curr_title\", \"left\")\n",
    "      .groupBy(\"curr_category\")\n",
    "      .agg(F.sum(\"n\").alias(\"total_n\"))\n",
    "      .orderBy(F.desc(\"total_n\")))\n",
    "j2.explain(\"formatted\")\n",
    "\n",
    "# Save one plan for evidence\n",
    "with open(\"proof/plan_broadcast.txt\",\"w\") as f:\n",
    "    from datetime import datetime as _dt\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(j2._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_broadcast.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006628a6",
   "metadata": {},
   "source": [
    "## 4. Additional queries for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b54b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: daily GMV‑like metric (sum of n) for a specific title window\n",
    "q2_row = (row_df.filter((F.col(\"type\")==\"link\") & F.col(\"curr_title\").isin(\"Apache_Spark\",\"PySpark\"))\n",
    "           .groupBy(\"year\",\"month\",\"curr_title\").agg(F.sum(\"n\").alias(\"n\")).orderBy(\"year\",\"month\",\"curr_title\"))\n",
    "q2_col = (col_df.filter((F.col(\"type\")==\"link\") & F.col(\"curr_title\").isin(\"Apache_Spark\",\"PySpark\"))\n",
    "           .groupBy(\"year\",\"month\",\"curr_title\").agg(F.sum(\"n\").alias(\"n\")).orderBy(\"year\",\"month\",\"curr_title\"))\n",
    "\n",
    "# Trigger\n",
    "_ = q2_row.count(); _ = q2_col.count()\n",
    "\n",
    "# Q3: heavy cardinality grouping\n",
    "q3_row = row_df.groupBy(\"prev_title\",\"curr_title\").agg(F.sum(\"n\").alias(\"n\")).orderBy(F.desc(\"n\")).limit(100)\n",
    "q3_col = col_df.groupBy(\"prev_title\",\"curr_title\").agg(F.sum(\"n\").alias(\"n\")).orderBy(F.desc(\"n\")).limit(100)\n",
    "_ = q3_row.count(); _ = q3_col.count()\n",
    "\n",
    "print(\"Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab3_metrics_log.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9eec0f",
   "metadata": {},
   "source": [
    "## 5. Save sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a78c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, pandas as pd\n",
    "pathlib.Path(\"outputs/lab3\").mkdir(parents=True, exist_ok=True)\n",
    "q1_row.limit(10).toPandas().to_csv(\"outputs/lab3/q1_row_top10.csv\", index=False)\n",
    "q1_col.limit(10).toPandas().to_csv(\"outputs/lab3/q1_col_top10.csv\", index=False)\n",
    "j2.limit(20).toPandas().to_csv(\"outputs/lab3/j2_broadcast_sample.csv\", index=False)\n",
    "print(\"Saved sample outputs in outputs/lab3/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b72a2",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ed171",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
